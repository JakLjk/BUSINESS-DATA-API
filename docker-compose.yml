services:
  redis:
    image: redis:latest
    ports:
      - 6379:6379
    restart: always

  api_endpoint:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: business_data_api
    ports:
      - "8078:8000"
    env_file:
      - .env
    depends_on:
      - redis

  worker-krsapi:
    build:
      context: .
      dockerfile: Dockerfile.api
    command: ["poetry", "run", "python", "run_worker.py", "KRSAPI"]
    # container_name: business_data_worker_krsapi
    env_file:
      - .env
    depends_on:
    # Eligible to change when stack will be adapted to wokr on multi-docker/host environment
      - redis
      - api_endpoint
  
  worker-krsdf:
    build:
      context: .
      dockerfile: Dockerfile.api
    command: ["poetry", "run", "python", "run_worker.py", "KRSDF"]
    # container_name: business_data_worker_krsdf
    env_file:
      - .env
    depends_on:
      - redis
      - api_endpoint

  automation-krsapi-refresh:
    build:
      context: .
      dockerfile:  Dockerfile.automation
    container_name: "krsapi_automation_refresh"
    env_file:
      - .env

  spark-etl:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: business_data_spark
    env_file:
      - .env
    # Necessary in order to connect to spark engine service
    network_mode: host
    volumes:
      - ${DOCKER_PERSISTENT_CHECKPOINT_PATH:-/tmp/spark_checkpoints}:/app/spark_etl/checkpoints

volumes:
  redis_registry_volume_1: